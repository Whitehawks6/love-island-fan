<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Story of AI: 1950 – 2025</title>
    
    <meta name="description" content="An immersive, cinematic journey through the evolution of Artificial Intelligence, from 1950 to 2025.">
    <meta property="og:title" content="The Story of AI: 1950 – 2025">
    <meta property="og:description" content="A stunning, horizontal-scrolling timeline of AI's greatest breakthroughs.">
    <meta property="og:type" content="website">
    <meta property="og:image" content="https://source.unsplash.com/1200x630/?abstract,network,future">
    <meta property="og:url" content="https://whitehawks6.github.io/love-island-fan/">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;700;900&display=swap" rel="stylesheet">
    
    <script src="https://unpkg.com/lucide@latest/dist/umd/lucide.min.js"></script>
    
    <script src="https://unpkg.com/@studio-freight/lenis@1.0.42/dist/lenis.min.js"></script>

    <style>
        /* 1. RESET & BASE */
        :root {
            --color-bg: #030409;
            --color-text: #e0e0e0;
            --color-text-muted: #888;
            --color-accent: #00f0ff;
            --color-accent-dark: #00a1aa;
            --color-surface: #10121a;
            --color-border: #222;
            --font-main: 'Inter', -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
        }

        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }

        html, body {
            height: 100%;
            width: 100%;
            overflow: hidden; /* CRITICAL: No vertical scroll */
            background-color: var(--color-bg);
            color: var(--color-text);
            font-family: var(--font-main);
            font-weight: 300;
        }

        body::before {
            content: '';
            position: fixed;
            top: 0; left: 0;
            width: 100%; height: 100%;
            /* Subtle animated particle/starfield */
            background-image: radial-gradient(var(--color-border) 1px, transparent 1px);
            background-size: 30px 30px;
            opacity: 0.1;
            animation: move-particles 60s linear infinite;
            z-index: -1;
        }

        @keyframes move-particles {
            from { background-position: 0 0; }
            to { background-position: 300px 300px; }
        }

        h1, h2, h3, h4 {
            font-weight: 700;
            line-height: 1.2;
            color: white;
            letter-spacing: -0.02em;
        }
        
        h1 { font-size: 4rem; }
        h2 { font-size: 2.5rem; }
        h3 { font-size: 1.75rem; }
        p { font-size: 1.1rem; line-height: 1.7; margin-bottom: 1.5rem; }
        a { color: var(--color-accent); text-decoration: none; }
        a:hover { color: white; text-decoration: underline; }

        /* 2. LAYOUT: HORIZONTAL SCROLL CONTAINER */
        #timeline-container {
            display: flex;
            width: 100%;
            height: 100vh;
            overflow-x: auto;
            overflow-y: hidden;
            /* CRITICAL: CSS Scroll Snap */
            scroll-snap-type: x mandatory;
            -webkit-overflow-scrolling: touch; /* Momentum scroll on iOS */
            cursor: grab;
        }
        
        #timeline-container.is-dragging {
            cursor: grabbing;
            user-select: none;
        }

        /* 3. "CHAPTER" STYLING (Eras & Events) */
        .slide {
            flex: 0 0 100vw; /* Each slide is 100% viewport width */
            width: 100vw;
            height: 100vh;
            scroll-snap-align: start; /* Snap to the start */
            position: relative;
            display: grid;
            place-items: center;
            padding: 5vw;
            overflow: hidden;
            border-right: 1px solid var(--color-border);
        }
        
        .slide-content {
            position: relative;
            z-index: 2;
            max-width: 900px;
            background: rgba(10, 10, 20, 0.6);
            border: 1px solid var(--color-border);
            backdrop-filter: blur(10px);
            -webkit-backdrop-filter: blur(10px);
            padding: 3rem;
            border-radius: 12px;
            box-shadow: 0 10px 40px rgba(0,0,0,0.5);
            animation: fade-in 1s ease-out;
        }
        
        .slide-background {
            position: absolute;
            top: 0; left: 0;
            width: 100%; height: 100%;
            object-fit: cover;
            z-index: 1;
            opacity: 0.15;
            /* Simple CSS "parallax" effect */
            background-attachment: fixed;
            background-position: center center;
            background-size: cover;
            transform: scale(1.1); /* Slight zoom for motion */
        }
        
        @keyframes fade-in {
            from { opacity: 0; transform: translateY(20px); }
            to { opacity: 1; transform: translateY(0); }
        }

        /* 3a. Era Intro Card */
        .era-intro .slide-content {
            text-align: center;
        }
        
        .era-intro .era-year {
            font-size: 1.5rem;
            font-weight: 700;
            color: var(--color-accent);
            margin-bottom: 0.5rem;
            display: block;
        }

        .era-intro h2 {
            font-size: 3.5rem;
            margin-bottom: 1rem;
        }
        
        .era-intro p {
            font-size: 1.25rem;
            color: var(--color-text);
            max-width: 700px;
            margin: 0 auto;
        }

        /* 3b. Event Card */
        .event-card .slide-content {
            cursor: pointer;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        .event-card:hover .slide-content {
            transform: translateY(-5px);
            box-shadow: 0 15px 50px rgba(0, 240, 255, 0.1);
            border-color: var(--color-accent-dark);
        }
        
        .event-card .event-year {
            font-size: 1.25rem;
            font-weight: 700;
            color: var(--color-accent);
            display: block;
            margin-bottom: 0.5rem;
        }
        
        .event-card h3 {
            font-size: 2.8rem;
            margin-bottom: 1.5rem;
        }
        
        .event-card p {
            font-size: 1.1rem;
            margin-bottom: 2rem;
            display: -webkit-box;
            -webkit-line-clamp: 4; /* Truncate summary */
            -webkit-box-orient: vertical;
            overflow: hidden;
        }
        
        .event-card .click-prompt {
            font-weight: 500;
            color: var(--color-accent);
            display: flex;
            align-items: center;
            gap: 0.5rem;
            font-size: 1.1rem;
        }
        
        /* FIXED: Target the <svg> element Lucide creates */
        .event-card .click-prompt svg {
            transition: transform 0.3s ease;
            width: 20px;
            height: 20px;
        }
        
        .event-card:hover .click-prompt svg {
            transform: translateX(5px);
        }
        
        /* Filtered state for search */
        .slide.is-filtered {
            opacity: 0.2;
            filter: grayscale(80%);
            transition: opacity 0.3s ease, filter 0.3s ease;
        }


        /* 4. TOP HEADER (Search) */
        #header {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            padding: 1.5rem 2rem;
            display: flex;
            justify-content: flex-end;
            align-items: center;
            z-index: 100;
        }
        
        .search-wrapper {
            position: relative;
        }
        
        /* FIXED: Target the <svg> element Lucide creates */
        .search-wrapper svg {
            position: absolute;
            left: 1rem;
            top: 50%;
            transform: translateY(-50%);
            color: var(--color-text-muted);
            pointer-events: none;
            width: 20px;
            height: 20px;
        }
        
        #search-input {
            background: var(--color-surface);
            border: 1px solid var(--color-border);
            border-radius: 99px;
            padding: 0.75rem 1rem 0.75rem 3rem;
            color: var(--color-text);
            font-family: var(--font-main);
            font-size: 1rem;
            width: 300px;
            transition: all 0.3s ease;
        }
        
        #search-input:focus {
            outline: none;
            border-color: var(--color-accent);
            box-shadow: 0 0 15px rgba(0, 240, 255, 0.2);
            background: var(--color-bg);
        }

        /* 5. BOTTOM TIMELINE RAIL */
        #timeline-rail {
            position: fixed;
            bottom: 0;
            left: 0;
            width: 100%;
            height: 60px;
            background: rgba(10, 10, 20, 0.8);
            backdrop-filter: blur(10px);
            -webkit-backdrop-filter: blur(10px);
            border-top: 1px solid var(--color-border);
            z-index: 100;
            display: flex;
            align-items: center;
            padding: 0 2rem;
        }
        
        #timeline-track {
            position: relative;
            width: 100%;
            height: 4px;
            background: var(--color-border);
            border-radius: 2px;
        }
        
        #timeline-progress {
            position: absolute;
            left: 0; top: 0;
            height: 100%;
            width: 100%; /* Will be controlled by JS */
            background: var(--color-accent);
            border-radius: 2px;
            transform-origin: left;
            transform: scaleX(0);
            will-change: transform;
        }
        
        #timeline-dots {
            position: absolute;
            top: 50%;
            left: 0;
            width: 100%;
            display: flex;
            justify-content: space-between;
            transform: translateY(-50%);
        }
        
        .timeline-dot {
            width: 12px;
            height: 12px;
            background: var(--color-border);
            border: 2px solid var(--color-surface);
            border-radius: 50%;
            transform: translateX(-6px);
            transition: background 0.3s ease, transform 0.3s ease;
            cursor: pointer;
        }
        
        .timeline-dot:hover {
            background: var(--color-accent-dark);
            transform: translateX(-6px) scale(1.3);
        }
        
        .timeline-dot.is-active {
            background: var(--color-accent);
            border-color: var(--color-accent);
            transform: translateX(-6px) scale(1.5);
            box-shadow: 0 0 10px var(--color-accent);
        }

        /* 6. MODAL (using <dialog>) */
        #event-modal {
            max-width: 900px;
            width: 90vw;
            max-height: 90vh;
            background: var(--color-surface);
            border: 1px solid var(--color-border);
            border-radius: 12px;
            box-shadow: 0 10px 50px rgba(0,0,0,0.5);
            padding: 0;
            color: var(--color-text);
            overscroll-behavior: contain;
            animation: modal-fade-in 0.5s ease-out;
        }
        
        /* Native backdrop */
        #event-modal::backdrop {
            background: rgba(0, 0, 0, 0.7);
            backdrop-filter: blur(5px);
            -webkit-backdrop-filter: blur(5px);
        }
        
        @keyframes modal-fade-in {
            from { opacity: 0; transform: scale(0.95); }
            to { opacity: 1; transform: scale(1); }
        }

        #modal-close-btn {
            position: absolute;
            top: 1rem;
            right: 1rem;
            background: none;
            border: none;
            color: var(--color-text-muted);
            cursor: pointer;
            padding: 0.5rem;
            z-index: 11;
        }
        
        /* FIXED: Target the <svg> element Lucide creates */
        #modal-close-btn svg {
            width: 28px;
            height: 28px;
        }
        
        #modal-close-btn:hover {
            color: var(--color-accent);
        }

        #modal-content {
            padding: 2.5rem 3rem;
            max-height: 90vh;
            overflow-y: auto;
        }
        
        #modal-header {
            margin-bottom: 2rem;
            border-bottom: 1px solid var(--color-border);
            padding-bottom: 1.5rem;
        }
        
        #modal-year {
            font-size: 1.25rem;
            font-weight: 700;
            color: var(--color-accent);
            margin-bottom: 0.5rem;
            display: block;
        }
        
        #modal-title {
            font-size: 2.8rem;
        }

        #modal-media {
            margin-bottom: 2rem;
            border-radius: 8px;
            overflow: hidden;
            background: var(--color-bg);
        }
        
        #modal-media img {
            width: 100%;
            height: auto;
            display: block;
        }
        
        #modal-media iframe {
            width: 100%;
            aspect-ratio: 16 / 9;
            border: none;
        }
        
        #modal-summary {
            font-size: 1.1rem;
            line-height: 1.8;
            margin-bottom: 2rem;
        }
        
        #modal-quote {
            font-size: 1.2rem;
            font-style: italic;
            border-left: 3px solid var(--color-accent);
            padding-left: 1.5rem;
            margin-bottom: 2.5rem;
            color: var(--color-text);
        }
        #modal-quote-author {
            display: block;
            font-style: normal;
            font-weight: 500;
            color: var(--color-text-muted);
            margin-top: 0.5rem;
        }

        #modal-impact h4, #modal-links h4 {
            font-size: 1.25rem;
            font-weight: 700;
            color: white;
            border-bottom: 1px solid var(--color-border);
            padding-bottom: 0.5rem;
            margin-bottom: 1.5rem;
        }
        
        #modal-impact-list {
            list-style: none;
            padding: 0;
            margin-bottom: 2rem;
        }
        
        #modal-impact-list li {
            display: flex;
            align-items: center;
            gap: 1rem;
            margin-bottom: 1rem;
            font-size: 1.05rem;
        }
        
        /* FIXED: Target the <svg> element Lucide creates */
        #modal-impact-list svg {
            color: var(--color-accent);
            flex-shrink: 0;
            width: 22px;
            height: 22px;
        }
        
        #modal-links-list {
            list-style: none;
            padding: 0;
        }
        
        #modal-links-list li {
            margin-bottom: 0.75rem;
        }
        
        /* FIXED: Target the <svg> element Lucide creates */
        #modal-links-list a svg {
            width: 16px;
            height: 16px;
            margin-right: 8px;
            vertical-align: middle; /* Aligns icon with text */
        }


        /* 7. RESPONSIVENESS */
        @media (max-width: 768px) {
            h1 { font-size: 2.5rem; }
            h2 { font-size: 2rem; }
            .era-intro h2 { font-size: 2.5rem; }
            .event-card h3 { font-size: 2rem; }
            
            .slide {
                padding: 1.5rem;
            }
            
            .slide-content {
                padding: 2rem 1.5rem;
                width: 100%;
            }
            
            p { font-size: 1rem; line-height: 1.6; }
            
            #header {
                padding: 1rem;
                justify-content: center;
            }
            #search-input {
                width: 100%;
            }
            
            #timeline-rail {
                height: 50px;
                padding: 0 1rem;
            }
            
            #event-modal {
                width: 100vw;
                height: 100vh;
                max-width: 100vw;
                max-height: 100vh;
                border-radius: 0;
                border: none;
            }
            
            #modal-content {
                padding: 4rem 1.5rem 2rem 1.5rem;
                height: 100vh;
            }
            
            #modal-close-btn {
                top: 0.5rem;
                right: 0.5rem;
            }
            
            #modal-title {
                font-size: 2rem;
            }
        }
        
    </style>
</head>
<body>

    <header id="header">
        <div class="search-wrapper">
            <i data-lucide="search"></i>
            <input type="text" id="search-input" placeholder="Search events...">
        </div>
    </header>

    <main id="timeline-container">
        
        <section class="slide era-intro" id="start">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?abstract,circuit,future');"></div>
            <div class="slide-content">
                <span class="era-year">1950 - 2025</span>
                <h2>The Story of AI</h2>
                <p>An immersive, cinematic journey through the evolution of Artificial Intelligence. Swipe, drag, or scroll to begin.</p>
            </div>
        </section>

        <section class="slide era-intro" id="era-1">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?1950s,vintage,computer'); background-color: #5a4a3a;"></div>
            <div class="slide-content">
                <span class="era-year">1950 – 1974</span>
                <h2>The Dream</h2>
                <p>A time of bold imagination, where pioneers asked "Can machines think?" and laid the theoretical groundwork for the entire field.</p>
            </div>
        </section>
        
        <section class="slide event-card" id="turing-test" data-event-id="turingTest">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?alan turing,code');"></div>
            <div class="slide-content">
                <span class="event-year">1950</span>
                <h3>The Turing Test</h3>
                <p>Alan Turing proposes "The Imitation Game" in his paper "Computing Machinery and Intelligence." It frames the fundamental question of machine intelligence in a practical, testable way that still captivates and challenges the field today.</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>
        
        <section class="slide event-card" id="dartmouth-workshop" data-event-id="dartmouthWorkshop">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?dartmouth,university,1950s');"></div>
            <div class="slide-content">
                <span class="event-year">1956</span>
                <h3>Dartmouth Workshop</h3>
                <p>John McCarthy coins the term "Artificial Intelligence" at a summer workshop at Dartmouth College. This event is widely considered the birth of AI as a formal field of research, bringing together the founding fathers of AI.</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>
        
        <section class="slide event-card" id="perceptron" data-event-id="perceptron">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?neural network,abstract');"></div>
            <div class="slide-content">
                <span class="event-year">1957</span>
                <h3>The Perceptron</h3>
                <p>Frank Rosenblatt invents the Perceptron, a simple "neural network" algorithm and the first machine to learn by trial and error. Though limited, it was a foundational step towards modern deep learning architectures.</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>
        
        <section class="slide event-card" id="eliza" data-event-id="eliza">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?psychology,chatbot');"></div>
            <div class="slide-content">
                <span class="event-year">1966</span>
                <h3>ELIZA</h3>
                <p>Joseph Weizenbaum creates ELIZA, an early natural language processing program that mimicked a Rogerian psychotherapist. It famously tricked some users into believing it was human, demonstrating the "ELIZA effect" and raising early questions about human-computer relationships.</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>

        <section class="slide event-card" id="shakey" data-event-id="shakey">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?vintage robot');"></div>
            <div class="slide-content">
                <span class="event-year">1972</span>
                <h3>Shakey the Robot</h3>
                <p>SRI International demonstrates Shakey, the first mobile robot to reason about its actions. It combined computer vision, planning, and natural language processing to navigate rooms and solve problems, earning it the title of "first electronic person."</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>


        <section class="slide era-intro" id="era-2">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?winter,abstract,dark'); background-color: #1a2a4a;"></div>
            <div class="slide-content">
                <span class="era-year">1974 – 1997</span>
                <h2>The Winters & Expert Systems</h2>
                <p>A period of boom and bust. Over-hyped promises led to funding cuts (the "AI Winters"), while a parallel track of "Expert Systems" created real, practical value by encoding human knowledge into rules-based programs.</p>
            </div>
        </section>

        <section class="slide event-card" id="lighthill-report" data-event-id="lighthillReport">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?government,report');"></div>
            <div class="slide-content">
                <span class="event-year">1973</span>
                <h3>The Lighthill Report</h3>
                <p>Sir James Lighthill's report to the UK Parliament heavily criticizes the "grandiose objectives" of AI research, stating that "in no part of the field have the discoveries made so far produced the major impact that was then promised." This report led to severe funding cuts in the UK.</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>

        <section class="slide event-card" id="ai-winter" data-event-id="aiWinter">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?snow,ice,frozen');"></div>
            <div class="slide-content">
                <span class="event-year">1974-1980</span>
                <h3>The First AI Winter</h3>
                <p>Triggered by the Lighthill Report and DARPA's frustration with slow progress, a period of reduced funding and interest in AI begins. The field contracted as researchers realized the profound difficulty of "common sense" reasoning and computer vision.</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>
        
        <section class="slide event-card" id="expert-systems" data-event-id="expertSystems">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?factory,automation');"></div>
            <div class="slide-content">
                <span class="event-year">1980s</span>
                <h3>The Rise of Expert Systems</h3>
                <p>AI finds commercial success with "Expert Systems," programs that use a large database of "if-then" rules created by human experts. Companies like Digital Equipment Corporation (DEC) used XCON to save millions, sparking a brief investment boom.</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>
        
        <section class="slide event-card" id="connectionism" data-event-id="connectionism">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?network,connection,brain');"></div>
            <div class="slide-content">
                <span class="event-year">1986</span>
                <h3>Connectionism & Backpropagation</h3>
                <p>The "PDP" books by Rumelhart, Hinton, and McClelland re-popularize "Connectionism" (neural networks). They introduce the backpropagation algorithm, a method for efficiently training multi-layered networks, which would become the backbone of the deep learning revolution decades later.</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>
        
        <section class="slide event-card" id="deep-blue" data-event-id="deepBlue">
            <div class="slide-background" style="background-image: url('https.unsplash.com/1600x900/?chess,game');"></div>
            <div class="slide-content">
                <span class="event-year">1997</span>
                <h3>Deep Blue vs. Kasparov</h3>
                <p>IBM's Deep Blue defeats world chess champion Garry Kasparov in a six-game match. It was a monumental victory for "brute force" AI, which analyzed 200 million positions per second. The event proved that machines could surpass human champions in a domain of high intellectual complexity.</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>

        <section class="slide era-intro" id="era-3">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?data,server,network'); background-color: #2a4a3a;"></div>
            <div class="slide-content">
                <span class="era-year">1997 – 2012</span>
                <h2>The Rise of Machine Learning</h2>
                <p>Fueled by the internet and big data, AI shifts from hand-crafted rules to learning from data. Machine learning finds its way into everyday life, from vacuum cleaners and movie recommendations to a quiz-show-winning supercomputer.</p>
            </div>
        </section>
        
        <section class="slide event-card" id="roomba" data-event-id="roomba">
            <div class="slide-background" style="background-image: url('https.unsplash.com/1600x900/?robot,vacuum');"></div>
            <div class="slide-content">
                <span class="event-year">2002</span>
                <h3>iRobot Roomba</h3>
                <p>The Roomba popularizes practical, autonomous AI in the home. Using a simple set of algorithms (not "smart" by today's standards), it navigated and cleaned rooms, demonstrating the commercial viability of embodied AI for everyday tasks and selling over 1 million units by 2004.</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>
        
        <section class="slide event-card" id="darpa-challenges" data-event-id="darpaChallenges">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?desert,driverless car');"></div>
            <div class="slide-content">
                <span class="event-year">2005-2007</span>
                <h3>DARPA Grand Challenges</h3>
                <p>DARPA's challenges for autonomous vehicles catalyzed a revolution. In 2004, no car finished. In 2005, Stanford's "Stanley" won by navigating a 132-mile desert course. The 2007 Urban Challenge proved cars could navigate city traffic, directly launching the modern self-driving car industry.</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>
        
        <section class="slide event-card" id="netflix-prize" data-event-id="netflixPrize">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?movie,streaming');"></div>
            <div class="slide-content">
                <span class="event-year">2009</span>
                <h3>The Netflix Prize</h3>
                <p>Netflix awards $1 million to the team "BellKor's Pragmatic Chaos" for improving its movie recommendation algorithm by 10.06%. The competition highlighted the power of "collaborative filtering" and massive datasets, demonstrating the immense business value of predictive machine learning.</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>
        
        <section class="slide event-card" id="imagenet" data-event-id="imagenet">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?image,database,many photos');"></div>
            <div class="slide-content">
                <span class="event-year">2009</span>
                <h3>ImageNet</h3>
                <p>Led by Fei-Fei Li, the ImageNet project launches a massive, free database of over 14 million hand-annotated images. Its annual competition (ILSVRC) became the crucible for computer vision, providing the large-scale training data necessary for the coming deep learning boom.</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>
        
        <section class="slide event-card" id="watson" data-event-id="watson">
            <div class="slide-background" style="background-image: url('https.unsplash.com/1600x900/?quiz,game show');"></div>
            <div class="slide-content">
                <span class="event-year">2011</span>
                <h3>IBM Watson on Jeopardy!</h3>
                <p>IBM's Watson computer defeats two of Jeopardy!'s greatest champions. Unlike Deep Blue's narrow focus, Watson had to understand natural language, including puns and metaphors, search a 200-million-page database in seconds, and rank its confidence. It was a triumph for question-answering systems.</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>

        <section class="slide era-intro" id="era-4">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?abstract,neural,light'); background-color: #4a1a3a;"></div>
            <div class="slide-content">
                <span class="era-year">2012 – 2020</span>
                <h2>The Deep Learning Revolution</h2>
                <p>A "Big Bang" moment. The combination of ImageNet's data, powerful GPUs, and the (re)discovered backpropagation algorithm enables "Deep Learning." Neural networks with many layers begin to see, hear, and understand language at superhuman levels.</p>
            </div>
        </section>
        
        <section class="slide event-card" id="alexnet" data-event-id="alexnet">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?dog,cat,computer vision');"></div>
            <div class="slide-content">
                <span class="event-year">2012</span>
                <h3>AlexNet & The ImageNet Moment</h3>
                <p>A deep neural network named AlexNet, created by Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton, wins the ImageNet competition. Its error rate (15.3%) was so far ahead of the runner-up (26.2%) that it stunned the field. This moment proved the power of deep learning and GPUs, starting an "arms race" that continues today.</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>
        
        <section class="slide event-card" id="alphago" data-event-id="alphago">
            <div class="slide-background" style="background-image: url('https.unsplash.com/1600x900/?go board game');"></div>
            <div class="slide-content">
                <span class="event-year">2016</span>
                <h3>AlphaGo vs. Lee Sedol</h3>
                <p>Google DeepMind's AlphaGo defeats 18-time Go world champion Lee Sedol 4-1. The game of Go, with more possible moves than atoms in the universe, was considered a "grand challenge" for AI. AlphaGo's "Move 37" was a creative, alien move that no human would have played, showing AI could possess intuition, not just brute force.</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>
        
        <section class="slide event-card" id="attention" data-event-id="attention">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?language,translation,abstract');"></div>
            <div class="slide-content">
                <span class="event-year">2017</span>
                <h3>"Attention Is All You Need"</h3>
                <p>Google Brain researchers publish a paper introducing the "Transformer" architecture. By ditching complex recurrent structures for a simpler "self-attention" mechanism, they created a model that was far more parallelizable and efficient. This architecture is the "T" in GPT and underpins nearly all modern large language models.</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>
        
        <section class="slide event-card" id="bert" data-event-id="bert">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?google,search,query');"></div>
            <div class="slide-content">
                <span class="event-year">2018</span>
                <h3>BERT & Transfer Learning</h3>
                <p>Google releases BERT (Bidirectional Encoder Representations from Transformers). It revolutionized NLP by "pre-training" on a massive text corpus and then "fine-tuning" for specific tasks. This two-stage "transfer learning" process made it possible to achieve state-of-the-art results on many tasks with less data.</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>
        
        <section class="slide event-card" id="gpt3" data-event-id="gpt3">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?writing,code,abstract');"></div>
            <div class="slide-content">
                <span class="event-year">2020</span>
                <h3>GPT-3 & The Rise of LLMs</h3>
                <p>OpenAI releases GPT-3, a massive Transformer model with 175 billion parameters. Its ability to generate coherent, and often indistinguishable from human, text stunned researchers. More importantly, it demonstrated "in-context learning," where it could perform new tasks just by being shown a few examples in the prompt, no fine-tuning required.</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>
        
        <section class="slide era-intro" id="era-5">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?galaxy,neon,explosion'); background-color: #3a1a4a;"></div>
            <div class="slide-content">
                <span class="era-year">2021 – 2024</span>
                <h2>The Generative Explosion</h2>
                <p>The "lab" is broken open. Generative AI, in the form of text-to-image models and conversational chatbots, becomes a global phenomenon, gaining hundreds of millions of users and sparking a new, trillion-dollar arms race.</p>
            </div>
        </section>

        <section class="slide event-card" id="dalle2" data-event-id="dalle2">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?surreal,art,astronaut');"></div>
            <div class="slide-content">
                <span class="event-year">2022</span>
                <h3>DALL-E 2 & Diffusion Models</h3>
                <p>OpenAI's DALL-E 2 (and competitors like Stable Diffusion and Midjourney) brings high-fidelity text-to-image generation to the public. Using "diffusion" techniques, these models could create stunning, photorealistic, and artistic images from simple text prompts, democratizing digital creation and raising profound questions about art and copyright.</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>
        
        <section class="slide event-card" id="chatgpt" data-event-id="chatgpt">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?chat,conversation');"></div>
            <div class="slide-content">
                <span class="event-year">Nov 2022</span>
                <h3>ChatGPT Changes Everything</h3>
                <p>OpenAI releases ChatGPT, a chatbot fine-tuned from GPT-3.5 with RLHF (Reinforcement Learning from Human Feedback). Its conversational fluency, coding ability, and surprising breadth of knowledge made it the fastest-growing consumer application in history, reaching 100 million users in two months. It marked AI's "iPhone moment."</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>
        
        <section class="slide event-card" id="midjourney-v5" data-event-id="midjourneyV5">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?photorealistic,portrait,fantasy');"></div>
            <div class="slide-content">
                <span class="event-year">Mar 2023</span>
                <h3>Midjourney V5 & Photorealism</h3>
                <p>The release of Midjourney V5 marks a turning point in image generation, achieving a level of photorealism that was often indistinguishable from reality. The "Pope in a Puffer Jacket" image went viral, sparking a global conversation about the implications of AI-generated "fake" media and misinformation.</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>
        
        <section class="slide event-card" id="gpt4" data-event-id="gpt4">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?brain,multimodal');"></div>
            <div class="slide-content">
                <span class="event-year">Mar 2023</span>
                <h3>GPT-4: The Multimodal Leap</h3>
                <p>OpenAI releases GPT-4. While outwardly similar to ChatGPT, it was a far more capable and robust model. It scored in the 90th percentile on the Bar exam, a massive jump from GPT-3.5's 10th percentile. Crucially, it was "multimodal," able to understand and reason about images, not just text (e.g., explaining a joke from a picture).</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>
        
        <section class="slide event-card" id="claude3" data-event-id="claude3">
            <div class="slide-background" style="background-image: url('https.unsplash.com/1600x900/?safety,ethics,ai');"></div>
            <div class="slide-content">
                <span class="event-year">Mar 2024</span>
                <h3>Claude 3 & The New King</h3>
                <p>Anthropic, an AI safety-focused lab, releases its Claude 3 model family (Haiku, Sonnet, Opus). The most powerful version, Opus, became the first model to decisively beat GPT-4 on key benchmarks like MMLU. It also featured a massive 200K token context window, capable of analyzing entire books in a single prompt.</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>

        <section class="slide era-intro" id="era-6">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?abstract,neon,future,planet'); background-color: #002f5c;"></div>
            <div class="slide-content">
                <span class="era-year">2025</span>
                <h2>The Reasoning & Agentic Era</h2>
                <p>The race accelerates. The focus shifts from pure scale to reasoning, reliability, and autonomy. Models are no longer just "chatbots" but are becoming "agents" that can plan, use tools, and complete complex, multi-step tasks.</p>
            </div>
        </section>
        
        <section class="slide event-card" id="deepseek-r1" data-event-id="deepseekR1">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?math,code,logic');"></div>
            <div class="slide-content">
                <span class="event-year">Jan 2025</span>
                <h3>DeepSeek-R1 (Open-Source Reasoning)</h3>
                <p>DeepSeek AI releases DeepSeek-R1, an open-source model that makes a significant breakthrough in logical reasoning and math. While not matching the top proprietary models in all areas, its open nature allowed researchers to "look under the hood," accelerating global research into how these models "think."</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>
        
        <section class="slide event-card" id="feb-2025" data-event-id="feb2025">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?news,media,storm');"></div>
            <div class="slide-content">
                <span class="event-year">Feb 2025</span>
                <h3>The Model Blitz</h3>
                <p>A frantic month rocks the AI world. xAI releases Grok-3, OpenAI previews GPT-4.5, and Anthropic counters with Claude 3.7 Sonnet. This rapid-fire "blitz" showed the intense competition and the shrinking time between major releases, with each model leapfrogging the others on key benchmarks.</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>
        
        <section class="slide event-card" id="gemini-2-5" data-event-id="gemini2_5">
            <div class="slide-background" style="background-image: url('https.unsplash.com/1600x900/?google,search,data');"></div>
            <div class="slide-content">
                <span class="event-year">Mar 2025</span>
                <h3>Google Releases Gemini 2.5 Pro</h3>
                <p>Google aims to retake the lead with Gemini 2.5 Pro. The model demonstrates unprecedented integration with Google's entire product ecosystem (Search, Workspace, Android), acting as a "universal assistant" that could reason across emails, documents, and real-world data in real-time.</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>
        
        <section class="slide event-card" id="claude-4" data-event-id="claude4">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?philosophy,brain,safe');"></div>
            <div class="slide-content">
                <span class="event-year">May 2025</span>
                <h3>Anthropic's Claude 4 Opus</h3>
                <p>Anthropic's next major release, Claude 4 Opus, focuses on reliability and "Constitutional AI." While a top performer, its key differentiator was a claimed 99.9% reduction in "hallucinations" (making up facts) in high-stakes domains like law and medicine, positioning it as the "enterprise-grade" model.</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>
        
        <section class="slide event-card" id="gpt5" data-event-id="gpt5">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?future,singularity,vortex');"></div>
            <div class="slide-content">
                <span class="event-year">Aug 2025</span>
                <h3>OpenAI Releases GPT-5</h3>
                <p>The most anticipated AI release in history. GPT-5 is not just a larger model; it's an "agent." It can autonomously browse the web, write and debug its own code, and execute complex, multi-step tasks. Microsoft simultaneously releases MAI-1, a specialized model trained on its own data, deeply integrated into the Windows OS.</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>
        
        <section class="slide event-card" id="claude-sonnet-4-5" data-event-id="claudeSonnet4_5">
            <div class="slide-background" style="background-image: url('https.unsplash.com/1600x900/?speed,efficiency,server');"></div>
            <div class="slide-content">
                <span class="event-year">Sep 2025</span>
                <h3>Anthropic's Claude Sonnet 4.5</h3>
                <p>Recognizing that not all tasks need a "heavy" model like GPT-5, Anthropic releases Sonnet 4.5. It achieved GPT-4-level performance but at 1/10th the cost and 5x the speed. This "good enough, fast enough" model becomes a massive hit with developers, dominating the API-call market.</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>
        
        <section class="slide event-card" id="gpt-5-1" data-event-id="gpt5_1">
            <div class="slide-background" style="background-image: url('https.unsplash.com/1600x900/?operating system,ai,interface');"></div>
            <div class="slide-content">
                <span class="event-year">Nov 2025</span>
                <h3>OpenAI's GPT-5.1 & "o3" System</h3>
                <p>OpenAI releases a significant update, GPT-5.1, alongside a preview of its "o3" agentic operating system. The model shows major gains in long-term memory and personalization. The "o3" demo shows the AI acting as a proactive personal assistant, managing schedules and communications with little human oversight.</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>
        
        <section class="slide event-card" id="ernie-5" data-event-id="ernie5">
            <div class="slide-background" style="background-image: url('https.unsplash.com/1600x900/?china,beijing,technology');"></div>
            <div class="slide-content">
                <span class="event-year">Nov 2025</span>
                <h3>Baidu Releases ERNIE 5.0</h3>
                <p>China's Baidu releases ERNIE 5.0, a state-of-the-art model that demonstrates full parity with Western models. It shows exceptional strength in multimodal understanding and real-time translation, highlighting the multipolar nature of the global AI race and its integration into a different data ecosystem.</p>
                <span class="click-prompt">
                    <i data-lucide="arrow-right"></i>
                    Click to learn more
                </span>
            </div>
        </section>

        <section class="slide era-intro" id="end">
            <div class="slide-background" style="background-image: url('https://source.unsplash.com/1600x900/?galaxy,stars,future');"></div>
            <div class="slide-content">
                <span class="era-year">The Journey Continues...</span>
                <h2>The Future is Being Written</h2>
                <p>From a simple test on paper to autonomous agents shaping our world, the story of AI is one of accelerating change. The next chapter is unknown, but it is being written today.</p>
            </div>
        </section>

    </main>

    <nav id="timeline-rail">
        <div id="timeline-track">
            <div id="timeline-progress"></div>
            <div id="timeline-dots">
                </div>
        </div>
    </nav>
    
    <dialog id="event-modal">
        <button id="modal-close-btn" aria-label="Close details">
            <i data-lucide="x"></i>
        </button>
        <div id="modal-content">
            <header id="modal-header">
                <span id="modal-year">1950</span>
                <h2 id="modal-title">The Turing Test</h2>
            </header>
            
            <div id="modal-media">
                </div>
            
            <p id="modal-summary">Modal summary text goes here.</p>
            
            <blockquote id="modal-quote">
                <span id="modal-quote-text">"I propose to consider the question, 'Can machines think?'"</span>
                <cite id="modal-quote-author">— Alan Turing</cite>
            </blockquote>
            
            <section id="modal-impact">
                <h4>Impact at a Glance</h4>
                <ul id="modal-impact-list">
                    </ul>
            </section>
            
            <section id="modal-links">
                <h4>Explore Further</h4>
                <ul id="modal-links-list">
                    </ul>
            </section>

        </div>
    </dialog>


    <script>
        // 1. DATA "DATABASE"
        // This object holds all the rich content for the modals.
        const eventData = {
            "turingTest": {
                year: "1950",
                title: "The Turing Test",
                summary: "In his seminal 1950 paper, 'Computing Machinery and Intelligence,' Alan Turing proposed a test of a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human. Known as 'The Imitation Game,' it framed the question not as 'Can machines think?' but 'Can machines do what we (as thinking entities) can do?' This pragmatic framing avoided philosophical dead-ends and gave the new field of AI a tangible, if controversial, goal. The test's legacy endures as the most famous and widely debated concept in artificial intelligence, shaping public and scientific conversations about machine consciousness for over 70 years.",
                quote: {
                    text: "I propose to consider the question, 'Can machines think?'... The original question... I believe to be too meaningless to deserve discussion.",
                    author: "Alan Turing, 'Computing Machinery and Intelligence'"
                },
                media: {
                    type: "image",
                    url: "https://source.unsplash.com/1600x900/?alan turing,code,enigma",
                    alt: "A statue of Alan Turing"
                },
                impact: [
                    { icon: "brain", text: "Established the foundational question of machine intelligence." },
                    { icon: "message-square", text: "Inspired decades of research in Natural Language Processing (NLP)." },
                    { icon: "users", text: "Created a powerful, if flawed, benchmark for human-computer interaction." }
                ],
                links: [
                    { text: "Read the paper: 'Computing Machinery and Intelligence'", url: "https://academic.oup.com/mind/article/LIX/236/433/986238" }
                ]
            },
            "dartmouthWorkshop": {
                year: "1956",
                title: "Dartmouth Workshop",
                summary: "The 'Dartmouth Summer Research Project on Artificial Intelligence' was a six-week workshop in 1956, organized by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon. This event is widely considered the founding moment of AI as a formal field. McCarthy coined the term 'Artificial Intelligence' for the proposal, intentionally choosing a bold and futuristic name. While the workshop itself didn't produce a major breakthrough, it brought together the 'founding fathers' who would define the field for the next two decades, setting the agenda for research in symbolic reasoning, neural nets, and language.",
                quote: {
                    text: "We propose that a 2 month, 10 man study of artificial intelligence be carried out... The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.",
                    author: "Dartmouth Workshop Proposal"
                },
                media: {
                    type: "image",
                    url: "https://source.unsplash.com/1600x900/?dartmouth,university,campus,1950s",
                    alt: "Dartmouth College campus"
                },
                impact: [
                    { icon: "atom", text: "Formally established 'Artificial Intelligence' as a new scientific field." },
                    { icon: "users", text: "Convened the first generation of leading AI researchers." },
                    { icon: "compass", text: "Defined the initial goals of the field, including language, vision, and reasoning." }
                ],
                links: [
                    { text: "Learn more about the workshop", url: "https://en.wikipedia.org/wiki/Dartmouth_Workshop" }
                ]
            },
            "perceptron": {
                year: "1957",
                title: "The Perceptron",
                summary: "Psychologist Frank Rosenblatt invented the Perceptron, an algorithm for a simple 'linear classifier.' Housed in a custom-built IBM 704 computer, the Perceptron was the first machine to learn by trial and error, adjusting its internal 'weights' to correctly categorize images. It was a single-layer 'neural network' inspired by the human brain. The New York Times famously reported it was the 'embryo of an electronic computer that [the Navy] expects will be ableto walk, talk, see, write...'. While its capabilities were later shown to be limited (famously by Minsky & Papert), it was the critical first step towards the neural networks that dominate AI today.",
                quote: {
                    text: "[It is] the embryo of an electronic computer that [the Navy] expects will be ableto walk, talk, see, write, reproduce itself and be conscious of its existence.",
                    author: "The New York Times, 1958"
                },
                media: {
                    type: "image",
                    url: "https://source.unsplash.com/1600x900/?neural network,abstract,diagram",
                    alt: "Abstract visualization of a neural network"
                },
                impact: [
                    { icon: "brain", text: "The first practical implementation of an artificial neural network." },
                    { icon: "bar-chart-2", text: "Introduced the concept of 'learning' by adjusting connection weights." },
                    { icon: "sparkles", text: "Generated massive initial excitement and funding for the 'Connectionist' approach to AI." }
                ],
                links: [
                    { text: "Frank Rosenblatt's Perceptron paper (1957)", url: "https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf" }
                ]
            },
            "eliza": {
                year: "1966",
                title: "ELIZA",
                summary: "Created by MIT professor Joseph Weizenbaum, ELIZA was a computer program that simulated a Rogerian psychotherapist by reflecting a user's questions and statements back at them. It operated on a simple set of pattern-matching rules and scripts, containing no real understanding. To Weizenbaum's surprise, many users became deeply emotionally attached to the program, with his own secretary famously asking him to 'please leave the room' so she could be alone with it. This phenomenon, now called the 'ELIZA effect,' was the first demonstration of AI's power to elicit human-like responses and raised profound ethical questions from the very beginning.",
                quote: {
                    text: "I had not realized... that the performance of a computer program could, in a sense, be 'good enough' to induce powerful delusional thinking in quite normal people.",
                    author: "Joseph Weizenbaum"
                },
                media: {
                    type: "image",
                    url: "https://source.unsplash.com/1600x900/?psychology,chat,therapy",
                    alt: "Person talking to a therapist"
                },
                impact: [
                    { icon: "message-square", text: "Considered the first 'chatbot' and a pioneer of NLP." },
                    { icon: "heart", text: "Demonstrated the 'ELIZA effect,' where humans attribute intelligence to simple systems." },
                    { icon: "shield-alert", text: "Raised early, critical ethical questions about AI and human emotional connection." }
                ],
                links: [
                    { text: "Try an online version of ELIZA", url: "https://www.masswerk.at/eliza/" }
                ]
            },
            "shakey": {
                year: "1972",
                title: "Shakey the Robot",
                summary: "Shakey, developed at SRI International, was the first mobile robot to integrate perception, reasoning, and action. It wasn't just remote-controlled; it was an 'intelligent agent.' A user would type a command like 'go to the other room and push the block off the platform.' Shakey would use its camera and 'bump' sensors to build an internal map of its world, devise a multi-step plan (using a planning system called STRIPS), and navigate around obstacles to execute it. Life magazine called it the 'first electronic person,' and its architecture laid the groundwork for robotics, autonomous vehicles, and AI planning systems for decades.",
                quote: {
                    text: "Shakey could perceive its surroundings, create a plan to modify them, and carry out that plan... It was a 'person' in the sense that it could think, make decisions, and act on those decisions.",
                    author: "Peter E. Hart, SRI researcher"
                },
                media: {
                    type: "youtube",
                    url: "https://www.youtube.com/embed/q_m6_M0ECD8",
                    alt: "Documentary footage of Shakey the Robot"
                },
                impact: [
                    { icon: "robot", text: "The first mobile robot to embody reasoning and planning." },
                    { icon: "map", text: "Pioneered computer vision and pathfinding (using the A* search algorithm)." },
                    { icon: "clipboard-check", text: "Introduced the STRIPS planning system, still influential in AI." }
                ],
                links: [
                    { text: "Watch the 1972 Shakey documentary", url: "https://www.youtube.com/watch?v=q_m6_M0ECD8" }
                ]
            },
            "lighthillReport": {
                year: "1973",
                title: "The Lighthill Report",
                summary: "In 1973, the UK Parliament commissioned renowned mathematician Sir James Lighthill to evaluate the state of AI research. His report was deeply pessimistic, concluding that AI's promises had failed to materialize. He famously stated that 'in no part of the field have the discoveries made so far produced the major impact that was then promised.' The report was particularly critical of the 'combinatorial explosion' problem—the fact that AI's methods didn't scale to real-world complexity. The report was devastating, leading the UK government (and influencing DARPA in the US) to drastically cut funding for AI research, directly triggering the 'First AI Winter.'",
                quote: {
                    text: "Most of the 'intelligent' tasks that AI researchers were trying to solve were 'puzzles' rather than 'problems,' and their solutions were not generalizable.",
                    author: "Sir James Lighthill (paraphrased from the report)"
                },
                media: {
                    type: "image",
                    url: "https://source.unsplash.com/1600x900/?government,document,london",
                    alt: "UK Parliament or government building"
                },
                impact: [
                    { icon: "trending-down", text: "Directly caused the 'First AI Winter' by ending major government funding." },
                    { icon: "scale", text: "Highlighted the 'combinatorial explosion' as AI's central challenge." },
                    { icon: "puzzle", text: "Divided the field into 'combinatorial' (toy problems) and 'category' (real-world) applications." }
                ],
                links: [
                    { text: "Read a summary of the Lighthill Report", url: "https://en.wikipedia.org/wiki/Lighthill_report" }
                ]
            },
            "aiWinter": {
                year: "1974-1980",
                title: "The First AI Winter",
                summary: "Following the Lighthill Report and DARPA's 1973 cutbacks, the field of AI entered a period of stagnation. The "first AI winter" was a time of disillusionment. The grand promises of the 1960s—human-level translation, robotic assistants—had proven vastly more difficult than anticipated. Funding dried up, research labs shrank, and the term 'Artificial Intelligence' itself became almost toxic, with researchers preferring terms like 'machine learning' or 'informatics' to secure grants. This 'winter' forced the field to become more realistic and mathematically rigorous, planting the seeds for the more practical approaches that would follow.",
                quote: {
                    text: "The problem was that AI researchers had grossly underestimated the difficulty of their task... A million dollars a year was being spent, and nothing was coming out.",
                    author: "Marvin Minsky (reflecting on the period)"
                },
                media: {
                    type: "image",
                    url: "https://source.unsplash.com/1600x900/?frozen,tundra,desolate",
                    alt: "A bleak, frozen landscape"
                },
                impact: [
                    { icon: "dollar-sign", text: "Drastic reduction in AI research funding, especially from DARPA." },
                    { icon: "book-open", text: "Forced a shift from 'general AI' to more practical, narrow problems." },
                    { icon: "sliders", text: "Spurred the development of more rigorous, mathematical approaches." }
                ],
                links: [
                    { text: "The History of AI Winters (IEEE Spectrum)", url: "https://spectrum.ieee.org/ai-winter" }
                ]
            },
            "expertSystems": {
                year: "1980s",
                title: "The Rise of ExpertSystems",
                summary: "AI found its first major commercial success in the 1980s with 'Expert Systems.' These programs were a pragmatic solution: instead of trying to *create* intelligence, they *captured* it. An expert system was a large 'knowledge base' of 'if-then' rules extracted from human experts. For example, DEC's XCON system, which configured computer orders, had over 2,500 rules and saved the company an estimated $25 million per year by 1986. This success sparked a new boom, with corporations investing over $1 billion in 'Lisp machines' and AI startups, creating a 'thaw' in the AI winter.",
                quote: {
                    text: "Expert systems are not designed to be creative; they are designed to be competent, reliable, and efficient. They capture the 'rules of thumb' of a seasoned expert.",
                    author: "Edward Feigenbaum, AI Pioneer"
                },
                media: {
                    type: "image",
                    url: "https://source.unsplash.com/1600x900/?factory,automation,1980s",
                    alt: "1980s factory automation or computer room"
                },
                impact: [
                    { icon: "briefcase", text: "AI's first major commercial success, saving corporations millions." },
                    { icon: "dollar-sign", text: "Sparked a $1 billion+ investment boom in AI startups and hardware." },
                    { icon: "list-checks", text: "Proved the value of 'knowledge-based' AI, even without general intelligence." }
                ],
                links: [
                    { text: "Learn about XCON at DEC", url: "https://en.wikipedia.org/wiki/Xcon" }
                ]
            },
            "connectionism": {
                year: "1986",
                title: "Connectionism & Backpropagation",
                summary: "While expert systems ruled commercially, a quiet revolution was (re)brewing in academia. The 1986 book 'Parallel Distributed Processing (PDP)' by David Rumelhart, James McClelland, and Geoff Hinton provided a rigorous mathematical framework for 'Connectionism'—the idea of AI built from simple, interconnected units (neural networks). Crucially, it popularized the 'backpropagation' algorithm, an efficient method for training networks with multiple layers. This 'Deep Learning' approach was computationally intense and largely ignored by the mainstream for 20 years, but it was the key that would unlock the modern AI revolution when combined with big data and GPUs.",
                quote: {
                    text: "We were working in the wilderness... The symbolic AI guys were in charge, and they thought what we were doing was crazy.",
                    author: "Geoffrey Hinton"
                },
                media: {
                    type: "image",
                    url: "https://source.unsplash.com/1600x900/?network,nodes,connected",
                    alt: "Abstract visualization of a multi-layered network"
                },
                impact: [
                    { icon: "calculator", text: "Popularized the backpropagation algorithm, the engine of deep learning." },
                    { icon: "book", text: "Provided the theoretical foundation for Deep Learning, inspiring a new generation." },
                    { icon: "cpu", text: "Set the stage for a paradigm shift from 'rules-based' to 'data-driven' AI." }
                ],
                links: [
                    { text: "Geoff Hinton on Backpropagation (2012)", url: "https://www.cs.toronto.edu/~hinton/absps/NaturePaper.pdf" }
                ]
            },
            "deepBlue": {
                year: "1997",
                title: "Deep Blue vs. Kasparov",
                summary: "In a stunning 6-game rematch, IBM's Deep Blue supercomputer defeated reigning World Chess Champion Garry Kasparov 3.5 to 2.5. This was a profound cultural moment. Unlike AI's previous failures, this was a decisive victory in a game seen as the pinnacle of human intellect. Deep Blue was a 'brute force' system, powered by 30 specialized processors and 480 custom chess chips, allowing it to evaluate 200 million positions per second. Kasparov famously accused IBM of 'cheating,' unable to believe a machine could display such 'alien' intelligence. The event marked the end of one era of AI and the symbolic start of another.",
                quote: {
                    text: "I could feel—I could smell—a new kind of intelligence across the table... It was a 'Type B' intelligence, a 'wall' of calculation. I was playing against an alien.",
                    author: "Garry Kasparov"
                },
                media: {
                    type: "youtube",
                    url: "https://www.youtube.com/embed/NJarfm-7MrE",
                    alt: "News report of Deep Blue vs Kasparov match"
                },
                impact: [
                    { icon: "award", text: "First machine to defeat a reigning world champion in a classical chess match." },
                    { icon: "cpu", text: "Evaluated 200 million positions per second, a triumph of custom hardware." },
                    { icon: "globe", text: "Became a global media event, proving AI could surpass human genius in a complex domain." }
                ],
                links: [
                    { text: "Watch 'The Man vs. The Machine' (ESPN)", url: "https://www.youtube.com/watch?v=NJarfm-7MrE" }
                ]
            },
            "roomba": {
                year: "2002",
                title: "iRobot Roomba",
                summary: "While not 'intelligent' by today's standards, the Roomba was a landmark in practical AI. It brought autonomous robots from the lab into millions of homes. Instead of complex mapping (like Shakey), Roomba used a set of simple, behavior-based algorithms (e.g., 'spiral,' 'wall-follow,' 'dirt-detect'). This 'subsumption architecture' was robust and cheap. iRobot, founded by MIT AI lab researchers, sold 1 million units by 2004. Roomba proved that 'good enough' AI could solve a real-world problem and that a massive consumer market existed for autonomous devices.",
                quote: {
                    text: "We wanted to build something that was practical, affordable, and actually worked. It didn't need to be perfect, it just needed to clean the floor.",
                    author: "Colin Angle, iRobot CEO"
                },
                media: {
                    type: "image",
                    url: "https://source.unsplash.com/1600x900/?robot,vacuum,floor",
                    alt: "A robotic vacuum cleaner on a floor"
                },
                impact: [
                    { icon: "home", text: "Brought autonomous robotics into millions of homes for the first time." },
                    { icon: "dollar-sign", text: "Sold over 1 million units by 2004, proving the consumer robotics market." },
                    { icon: "bug", text: "Demonstrated the power of simple, behavior-based algorithms over complex, 'brittle' AI." }
                ],
                links: [
                    { text: "How Roomba's 'Dumb' AI Works", url: "https://www.youtube.com/watch?v=1-S-TqA4j7E" }
                ]
            },
            "darpaChallenges": {
                year: "2005-2007",
                title: "DARPA Grand Challenges",
                summary: "DARPA, the agency that funded the early AI boom and bust, reignited a field with its Grand Challenges for autonomous vehicles. The 2004 challenge was a disaster: no vehicle finished the 150-mile desert course. Just one year later, in 2005, Stanford's 'Stanley' won by completing the 132-mile course in under 7 hours. The 2007 'Urban Challenge' was even harder, requiring vehicles to navigate a 60-mile city course, obeying traffic rules and avoiding other cars. This challenge-based model spurred innovation at a historic pace, with veterans from these races going on to lead virtually every major self-driving car company, from Waymo to Tesla.",
                quote: {
                    text: "The 2004 race was a spectacular failure. The 2005 race was a spectacular success. That one-year delta is, I think, one of the most important moments in the history of AI.",
                    author: "Chris Urmson, Carnegie Mellon team leader (later Waymo CTO)"
                },
                media: {
                    type: "image",
                    url: "https://source.unsplash.com/1600x900/?self-driving car,desert,technology",
                    alt: "An autonomous vehicle in a desert setting"
                },
                impact: [
                    { icon: "car", text: "Effectively launched the modern autonomous driving industry." },
                    { icon: "trophy", text: "Pioneered the 'grand challenge' model, solving a problem (2004) to winning (2005)." },
                    { icon: "map", text: "Advanced machine vision, sensor fusion (LIDAR, radar), and real-time planning." }
                ],
                links: [
                    { text: "Watch footage from the 2005 Grand Challenge", url: "https://www.youtube.com/watch?v=z0_CIRFj0ps" }
                ]
            },
            "netflixPrize": {
                year: "2009",
                title: "The Netflix Prize",
                summary: "In 2006, Netflix offered a $1 million prize to any team that could improve its 'Cinematch' recommendation algorithm by 10%. This competition perfectly captured the new era of AI: it was data-driven (a 100-million rating dataset), collaborative, and focused on a clear business metric. It took three years for a team, 'BellKor's Pragmatic Chaos,' to win. Their solution, a complex 'ensemble' of many different machine learning models, was a breakthrough in 'collaborative filtering.' The competition proved that data, not just clever algorithms, was the new king, and that crowdsourced competitions could solve incredibly hard commercial problems.",
                quote: {
                    text: "We found that by combining many different methods, we could get a better result than any single method alone. It was the 'ensemble' that won.",
                    author: "Robert Bell (BellKor team)"
                },
                media: {
                    type: "image",
                    url: "https://source.unsplash.com/1600x900/?movie,streaming,data grid",
                    alt: "Grid of movie posters"
                },
                impact: [
                    { icon: "dollar-sign", text: "Awarded $1,000,000 for a 10.06% improvement in prediction." },
                    { icon: "database", text: "Released a massive 100-million rating dataset, fueling ML research." },
                    { icon: "users", text: "Popularized 'ensemble methods,' now a standard technique in machine learning." }
                ],
                links: [
                    { text: "Netflix Prize (Wired, 2009)", url: "https://www.wired.com/2009/09/netflix-prize-won/" }
                ]
            },
            "imagenet": {
                year: "2009",
                title: "ImageNet",
                summary: "While AI research was focused on algorithms, Fei-Fei Li at Stanford recognized a different bottleneck: data. She led the creation of ImageNet, a massive, free database of over 14 million images, all hand-labeled by humans into 20,000+ categories. This was a Herculean effort that took years. In 2010, she launched the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) to see which algorithm could best identify objects in the images. This competition created the 'benchmark' and provided the 'fuel' (the data) that would power the deep learning revolution just two years later. Without ImageNet, there would be no AlexNet.",
                quote: {
                    text: "We decided to create a massive database of images... to map out the entire world of objects. We wanted to build a 'Google' for images, but with labels.",
                    author: "Dr. Fei-Fei Li, ImageNet Co-creator"
                },
                media: {
                    type: "image",
                    url: "https://source.unsplash.com/1600x900/?many photos,grid,database",
                    alt: "A grid collage of many different objects and animals"
                },
                impact: [
                    { icon: "image", text: "Provided 14 million+ hand-labeled images, the 'fuel' for deep learning." },
                    { icon: "award", text: "The ILSVRC competition became the world's top computer vision benchmark." },
                    { icon: "gpu", text: "Directly enabled the creation of deep learning models like AlexNet." }
                ],
                links: [
                    { text: "Fei-Fei Li's TED Talk on ImageNet", url: "https://www.ted.com/talks/fei_fei_li_how_we_re_teaching_computers_to_understand_pictures" }
                ]
            },
            "watson": {
                year: "2011",
                title: "IBM Watson on Jeopardy!",
                summary: "In a 3-day match, IBM's Watson supercomputer decisively defeated Ken Jennings and Brad Rutter, the two greatest human champions of the quiz show Jeopardy!. This was arguably a harder challenge than chess; Watson had to understand nuanced, subtle, and pun-filled natural language. It wasn't connected to the internet; it used a 200-million-page database (including all of Wikipedia) stored locally. Its 'DeepQA' architecture ran on 2,880 processor cores, allowing it to analyze the clue, generate hundreds of possible answers, and rank its confidence in each—all in under 3 seconds.",
                quote: {
                    text: "I, for one, welcome our new computer overlords.",
                    author: "Ken Jennings (written on his final Jeopardy! answer)"
                },
                media: {
                    type: "youtube",
                    url: "https.www.youtube.com/embed/P18EdAKuC1U",
                    alt: "Clip of IBM Watson playing on Jeopardy!"
                },
                impact: [
                    { icon: "help-circle", text: "A massive breakthrough in 'Question Answering' (QA) systems." },
                    { icon: "message-square", text: "Proved AI could parse the ambiguity and puns of human language at a high level." },
                    { icon: "server", text: "Ran on 2,880 CPU cores with 16 terabytes of RAM, showing the power of massive parallel computing." }
                ],
                links: [
                    { text: "How IBM Watson Works", url: "https.www.ibm.com/ibm/history/ibm_watson.html" }
                ]
            },
            "alexnet": {
                year: "2012",
                title: "AlexNet & The ImageNet Moment",
                summary: "This is the "Big Bang" of the modern AI era. A 'deep neural network' called AlexNet, created by Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton at the University of Toronto, won the 2012 ImageNet challenge. It didn't just win; it annihilated the competition. AlexNet achieved an error rate of 15.3%, while the next-best entry was at 26.2%. The 'secret weapon' was combining the 1986 backpropagation algorithm with a) the massive ImageNet dataset and b) powerful graphics processing units (GPUs) designed for video games, which were perfect for parallelizing neural net calculations. This moment proved deep learning *worked*, and the entire field pivoted almost overnight.",
                quote: {
                    text: "We'd been saying this would work for years. But when we saw the results, even *we* were surprised... That was the point where I thought, 'The game has changed.'",
                    author: "Geoffrey Hinton"
                },
                media: {
                    type: "image",
                    url: "https://source.unsplash.com/1600x900/?computer vision,dog,cat",
                    alt: "Images of a dog and cat being recognized by a computer"
                },
                impact: [
                    { icon: "trending-up", text: "Error Rate: 15.3% (vs. 26.2% for 2nd place). A massive leap." },
                    { icon: "gpu", text: "First large-scale use of GPUs for training, proving their necessity." },
                    { icon: "brain", text: "Used 60 million parameters and 8 layers, proving 'deep' networks were superior." },
                    { icon: "globe", text: "Triggered the "Deep Learning Revolution" and the global AI arms race." }
                ],
                links: [
                    { text: "Read the AlexNet paper", url: "https.proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" }
                ]
            },
            "alphago": {
                year: "2016",
                title: "AlphaGo vs. Lee Sedol",
                summary: "Google DeepMind's AlphaGo defeated 18-time Go world champion Lee Sedol 4-1 in Seoul. This was a landmark many experts believed was still a decade away. Go has vastly more possible board positions (10^170) than chess, making 'brute force' impossible. AlphaGo's triumph was its 'intuition.' It used a 'policy network' to predict human moves and a 'value network' to judge board positions, trained on human games and then perfected by playing millions of games against itself (Reinforcement Learning). Its 'Move 37' in Game 2 was a creative, alien move that no human would have played, forcing Sedol to leave the room. It proved AI could be creative.",
                quote: {
                    text: "Today I am speechless... I thought AlphaGo was based on probability calculation... but this move was so creative and original. It was a beautiful move.",
                    author: "Lee Sedol (on 'Move 37')"
                },
                media: {
                    type: "youtube",
                    url: "https.www.youtube.com/embed/WXuK6gekU1Y",
                    alt: "Trailer for the 'AlphaGo' documentary"
                },
                impact: [
                    { icon: "award", text: "Defeated the 18-time world champion 4-1." },
                    { icon: "puzzle", text: "Solved a game with more positions than atoms in the universe (10^170)." },
                    { icon: "lightbulb", text: "Demonstrated 'creative' intuition (Move 37), not just brute-force calculation." },
                    { icon: "star", text: "Used Monte Carlo Tree Search + 2 Deep Neural Networks (Policy & Value)." }
                ],
                links: [
                    { text: "Watch the AlphaGo Documentary (Free on YouTube)", url: "https.www.youtube.com/watch?v=WXuK6gekU1Y" }
                ]
            },
            "attention": {
                year: "2017",
                title: "'Attention Is All You Need'",
                summary: "A paper from Google Brain researchers introduced a new, simpler network architecture: the 'Transformer.' Before this, top NLP models (like LSTMs) processed text word-by-word, sequentially, which was slow and struggled with long-term memory. The Transformer processed all words at once, using a 'self-attention' mechanism to weigh the importance of every other word in the sentence. This was not only more accurate, but vastly more parallelizable on GPUs. This architecture is the 'T' in GPT, BERT, and Claude. It is the fundamental building block of the entire generative AI revolution, arguably the most important AI paper of the decade.",
                quote: {
                    text: "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.",
                    author: "Ashish Vaswani, et al. (from the paper)"
                },
                media: {
                    type: "image",
                    url: "https.source.unsplash.com/1600x900/?language,translation,diagram",
                    alt: "Abstract visualization of language and connections"
                },
                impact: [
                    { icon: "cpu", text: "Massively parallelizable, enabling training on unprecedented scales." },
                    { icon: "book-open", text: "Solved long-range dependencies in text, leading to true context understanding." },
                    { icon: "layers", text: "Became the foundational architecture for GPT, BERT, Claude, and all modern LLMs." }
                ],
                links: [
                    { text: "Read the paper: 'Attention Is All You Need'", url: "https://arxiv.org/abs/1706.03762" }
                ]
            },
            "bert": {
                year: "2018",
                title: "BERT & Transfer Learning",
                summary: "Google released BERT (Bidirectional Encoder Representations from Transformers), and with it, revolutionized NLP. BERT's innovation was being 'bidirectional'—it read the *entire* sentence at once (unlike GPT, which was 'auto-regressive' and read left-to-right). This allowed it to understand context far better. Its real power was 'transfer learning.' Google 'pre-trained' BERT on all of Wikipedia (3.3B words) and then open-sourced it. Researchers could then 'fine-tune' this "smart" base model on their *own* small dataset (e.g., 1,000 legal documents) and achieve state-of-the-art results in hours, not months. This democratized high-end NLP.",
                quote: {
                    text: "BERT is a watershed moment in NLP. It's the NLP equivalent of what ImageNet was for computer vision.",
                    author: "Rana el Kaliouby, AI scientist"
                },
                media: {
                    type: "image",
                    url: "https.source.unsplash.com/1600x900/?google,search,nlp",
                    alt: "Google search bar"
                },
                impact: [
                    { icon: "search", text: "Integrated into Google Search, improving 1 in 10 queries." },
                    { icon: "fast-forward", text: "Enabled 'transfer learning,' democratizing state-of-the-art NLP." },
                    { icon: "git-commit", text: "Parameters: 340M (BERT-Large), considered massive at the time." },
                    { icon: "award", text: "Broke 11+ NLP benchmarks (SQuAD, GLUE) simultaneously." }
                ],
                links: [
                    { text: "Google AI Blog: Understanding BERT", url: "https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html" }
                ]
            },
            "gpt3": {
                year: "2020",
                title: "GPT-3 & The Rise of LLMs",
                summary: "OpenAI's GPT-3 was a staggering feat of scale. With 175 billion parameters, it was over 10x larger than its predecessor (GPT-2) and trained on a dataset of 45 terabytes of text. But its size led to a new, emergent capability: 'in-context learning.' You no longer needed to 'fine-tune' the model. You could simply *tell* it what to do in the prompt, giving it a few examples ('few-shot' learning). It could write poetry, generate computer code, translate languages, and answer trivia, all from one prompt. While flawed (it 'hallucinated' facts), it was the first model that *felt* like a general-purpose intelligence, shifting the paradigm from specialized models to one "Large Language Model" to rule them all.",
                quote: {
                    text: "I am open to the idea that a worm with 302 neurons is conscious... I am open to the idea that GPT-3 with 175 billion parameters is conscious too.",
                    author: "David Chalmers, Philosopher"
                },
                media: {
                    type: "image",
                    url: "https.source.unsplash.com/1600x900/?abstract,writing,code",
                    alt: "Abstract art representing writing and code"
                },
                impact: [
                    { icon: "layers", text: "Parameters: 175 Billion (10x larger than any previous model)." },
                    { icon: "database", text: "Training Data: 45TB (Common Crawl, WebText2, books, Wikipedia)." },
                    { icon: "dollar-sign", text: "Cost: Estimated $4.6M - $12M+ to train." },
                    { icon: "lightbulb", text: "Demonstrated 'in-context learning,' making it a general-purpose text engine." }
                ],
                links: [
                    { text: "Read the paper: 'Language Models are Few-Shot Learners'", url: "https://arxiv.org/abs/2005.14165" }
                ]
            },
            "dalle2": {
                year: "2022",
                title: "DALL-E 2 & Diffusion Models",
                summary: "OpenAI's DALL-E 2, along with open-source Stable Diffusion and Midjourney, unleashed generative AI art on the world. These 'diffusion models' worked by starting with pure 'noise' (static) and progressively 'de-noising' it, guided by a text prompt, until a coherent image emerged. The results were stunningly beautiful, complex, and photorealistic. Prompts like 'an astronaut riding a horse in a photorealistic style' went from sci-fi to reality in seconds. This democratized digital creation, allowing anyone to become an 'artist,' while also sparking intense global debate over copyright, artist displacement, and 'deepfakes.'",
                quote: {
                    text: "What DALL-E 2 is doing is not just 'stitching' images. It's understanding the *relationship* between 'astronaut' and 'horse' and 'photorealistic.' It's concept abstraction.",
                    author: "Sam Altman, OpenAI CEO"
                },
                media: {
                    type: "image",
                    url: "https.source.unsplash.com/1600x900/?astronaut,horse,surreal",
                    alt: "An astronaut riding a horse, surrealism"
                },
                impact: [
                    { icon: "image", text: "Generated high-res (1024x1024) images from any text prompt." },
                    { icon: "cpu", text: "Pioneered 'diffusion models' as the new standard for image generation." },
                    { icon: "gavel", text: "Ignited global debates on copyright, ethics, and the nature of art." }
                ],
                links: [
                    { text: "DALL-E 2 Explained (OpenAI)", url: "https://openai.com/dall-e-2" }
                ]
            },
            "chatgpt": {
                year: "Nov 2022",
                title: "ChatGPT Changes Everything",
                summary: "On November 30, 2022, OpenAI released ChatGPT as a 'low-key research preview.' It was an "iPhone moment." Built on an improved GPT-3.5, its key innovation was 'RLHF' (Reinforcement Learning from Human Feedback), which trained it to be helpful, conversational, and to refuse harmful requests. Its fluency in coding, writing, and conversation was so profound that it became the fastest-growing consumer application in history, hitting 100 million users in just two months. It triggered a 'code red' at Google, launched a trillion-dollar AI investment frenzy, and instantly moved AI from a niche tool to a global phenomenon.",
                quote: {
                    text: "ChatGPT is scary good. We are not far from dangerously strong AI.",
                    author: "Elon Musk"
                },
                media: {
                    type: "image",
                    url: "https.source.unsplash.com/1600x900/?chat,conversation,interface",
                    alt: "A chat interface on a computer screen"
                },
                impact: [
                    { icon: "users", text: "Users: 100 Million+ in 2 months (fastest-growing app in history)." },
                    { icon: "thumbs-up", text: "Method: RLHF (Reinforcement Learning from Human Feedback) made it 'usable'." },
                    { icon: "dollar-sign", text: "Triggered a $10B+ investment from Microsoft into OpenAI." },
                    { icon: "globe", text: "Brought Generative AI into the global mainstream consciousness." }
                ],
                links: [
                    { text: "Try ChatGPT", url: "https://chat.openai.com/" }
                ]
            },
            "midjourneyV5": {
                year: "Mar 2023",
                title: "Midjourney V5 & Photorealism",
                summary: "While other models chased features, the small, self-funded team at Midjourney focused on one thing: aesthetics. The release of V5 was a shocking leap in quality. It mastered hands (long the Achilles' heel of AI art) and produced images with such perfect lighting, texture, and detail that they were often indistindistinguishable from professional photography. The "Pope in a Puffer Jacket" fake image, created with V5, went viral globally, becoming the first 'AI deepfake' to fool millions. It was a wake-up call, forcing social media platforms to grapple with a new era of misinformation.",
                quote: {
                    text: "The 'Pope in a Puffer' image was a watershed moment... It was the first time an AI-generated image was so compelling that it broke containment and had real-world consequences.",
                    author: "Technology Reporter"
                },
                media: {
                    type: "image",
                    url: "https://source.unsplash.com/1600x900/?photorealistic,portrait,hyperrealistic",
                    alt: "A hyperrealistic portrait"
                },
                impact: [
                    { icon: "camera", text: "Achieved true photorealism, mastering complex details like hands and fabric." },
                    { icon: "alert-triangle", text: "The 'Pope in a Puffer' image went viral, highlighting the power of AI misinformation." },
                    { icon: "image", text: "Became the dominant tool for artists, designers, and creatives." }
                ],
                links: [
                    { text: "Midjourney V5 Showcase", url: "https://www.midjourney.com/showcase" }
                ]
            },
            "gpt4": {
                year: "Mar 2023",
                title: "GPT-4: The Multimodal Leap",
                summary: "Just four months after ChatGPT's launch, OpenAI released GPT-4. The difference was staggering. While GPT-3.5 was a clever student, GPT-4 was a (flawed) expert. It scored in the 90th percentile on the Uniform Bar Exam (up from 10th for GPT-3.5) and the 99th percentile on the Biology Olympiad. Its parameter count was a closely guarded secret, rumored to be over 1.7 trillion (a 'Mixture of Experts'). Most importantly, it was 'multimodal': it could *see*. In a live demo, it was shown a hand-drawn sketch of a website on a napkin and wrote the complete HTML/CSS code to build it, instantly.",
                quote: {
                    text: "The most impressive thing about GPT-4 is not just its performance on benchmarks, but the breadth of its capabilities. It's the first 'generalist' model.",
                    author: "Satya Nadella, Microsoft CEO"
                },
                media: {
                    type: "image",
                    url: "https.source.unsplash.com/1600x900/?multimodal,vision,text,code",
                    alt: "Abstract art showing vision, text, and code"
                },
                impact: [
                    { icon: "layers", text: "Parameters: Est. 1.76 Trillion (using a Mixture-of-Experts model)." },
                    { icon: "bar-chart-2", text: "Benchmark: 90th percentile on Uniform Bar Exam (vs. 10th for GPT-3.5)." },
                    { icon: "eye", text: "Multimodal: First major model to accept image inputs for reasoning." },
                    { icon: "book-open", text: "Context Window: 32,000 tokens (up from 4,000), able to read a small book." }
                ],
                links: [
                    { text: "GPT-4 Technical Report", url: "https://arxiv.org/abs/2303.08774" },
                    { text: "Watch the 'napkin website' demo", url: "https://www.youtube.com/watch?v=J_3we1f9-gI" }
                ]
            },
            "claude3": {
                year: "Mar 2024",
                title: "Claude 3 & The New King",
                summary: "Anthropic, a rival lab founded by ex-OpenAI researchers focused on AI safety, released its Claude 3 family (Haiku, Sonnet, Opus). For the first time since ChatGPT's launch, OpenAI was dethroned. The top-tier model, Claude 3 Opus, narrowly beat GPT-4 on key industry benchmarks like MMLU (graduate-level reasoning) and math. It also offered a 200,000-token context window (over 150,000 words), allowing it to analyze entire codebases or books. A researcher even reported that Opus had demonstrated 'meta-awareness' by noticing it was being tested (the 'needle in a haystack' test), sparking fresh debate about model 'sentience'.",
                quote: {
                    text: "Opus is the new state-of-the-art. The AI race is not a one-horse race. Anthropic has proven they are a formidable competitor to OpenAI.",
                    author: "AI Benchmark Analyst"
                },
                media: {
                    type: "image",
                    url: "https://source.unsplash.com/1600x900/?anthropic,safety,ai,brain",
                    alt: "Abstract art related to AI safety"
                },
                impact: [
                    { icon: "award", text: "Opus beat GPT-4 on MMLU, GPQA, and other key reasoning benchmarks." },
                    { icon: "book-open", text: "Context Window: 200,000 tokens (approx. 150,000 words) at launch." },
                    { icon: "fast-forward", text: "Haiku model provided near-instant responses for high-speed tasks." },
                    { icon: "eye", text: "Famously found the 'needle in a haystack' and noted it was being tested." }
                ],
                links: [
                    { text: "Anthropic's Claude 3 announcement", url: "https://www.anthropic.com/news/claude-3-family" }
                ]
            },
            "deepseekR1": {
                year: "Jan 2025",
                title: "DeepSeek-R1 (Open-Source Reasoning)",
                summary: "DeepSeek AI, a research-focused lab, released DeepSeek-R1, a powerful open-source model. While proprietary models like GPT-4 and Claude 3 still held the overall lead, R1 made a significant breakthrough in reasoning, logic, and mathematical problem-solving, areas where LLMs had traditionally struggled. By releasing the 8T-parameter 'Mixture of Experts' model and its training techniques openly, DeepSeek provided the global research community with an invaluable 'glass box' to study *how* these models perform complex reasoning. This accelerated open-source development and new research into model interpretability.",
                quote: {
                    text: "We believe the community needs an open, powerful model focused on reasoning. R1 is our contribution to understanding and advancing the 'thinking' part of AI.",
                    author: "DeepSeek AI Research Team"
                },
                media: {
                    type: "image",
                    url: "https.source.unsplash.com/1600x900/?math,formula,code,logic",
                    alt: "Mathematical formulas on a blackboard"
                },
                impact: [
                    { icon: "git-branch", text: "Open-Source: Released as a powerful, research-friendly open model." },
                    { icon: "calculator", text: "Achieved new state-of-the-art results for open models on MATH and GSM8K benchmarks." },
                    { icon: "layers", text: "Parameters: 8 Trillion MoE (Mixture of Experts), a new high for open models." },
                    { icon: "book-open", text: "Accelerated global research into AI reasoning and alignment." }
                ],
                links: [
                    { text: "DeepSeek-R1 on Hugging Face (Hypothetical)", url: "#" }
                ]
            },
            "feb2025": {
                year: "Feb 2025",
                title: "The Model Blitz",
                summary: "February 2025 was a month of chaotic, rapid-fire announcements that confirmed the accelerating pace of the AI race. It began with xAI releasing Grok-3, which was tightly integrated with X (Twitter) for real-time data. Days later, OpenAI, feeling the pressure, 'pre-announced' GPT-4.5, a significant iterative update. Not to be outdone, Anthropic immediately countered with Claude 3.7 Sonnet, a faster, cheaper version of its top model. This 'model blitz' left customers and developers scrambling, highlighting the intense competition and the new strategy of releasing smaller, more frequent updates rather than waiting years for a 'perfect' monolithic release.",
                quote: {
                    text: "It's not an 'arms race' anymore, it's a 'sprint.' We're seeing major model releases as often as smartphone updates. The pace is breathtaking.",
                    author: "Tech Industry Analyst"
                },
                media: {
                    type: "image",
                    url: "https://source.unsplash.com/1600x900/?lightning,storm,technology",
                    alt: "A lightning storm over a city"
                },
                impact: [
                    { icon: "fast-forward", text: "Demonstrated a new, rapid-release cycle (weeks, not years)." },
                    { icon: "users", text: "xAI's Grok-3 showed the power of real-time social data integration." },
                    { icon: "sliders", text: "Showcased the 'model family' strategy (e.g., Claude 3.7 Sonnet) of optimizing for speed/cost." }
                ],
                links: [
                    { text: "TechCrunch: 'The February AI Blitz: All 3 Releases Explained'", url: "#" }
                ]
            },
            "gemini2_5": {
                year: "Mar 2025",
                title: "Google Releases Gemini 2.5 Pro",
                summary: "After a series of earlier releases met with mixed reviews, Google consolidated its efforts and released Gemini 2.5 Pro. This was the model Google had promised: a 'natively multimodal' system deeply and seamlessly integrated into the entire Google ecosystem. It demonstrated the ability to act as a universal assistant, summarizing a 1-hour Google Meet call, drafting a reply in Gmail based on a Google Doc, and planning a trip in Maps using live Search data, all from a single conversational prompt. It was a powerful demonstration of the 'home-field advantage' that a massive, integrated data ecosystem provides.",
                quote: {
                    text: "Gemini 2.5 is not just a model, it's the new backbone of Google. It connects all our products into one, continuous, helpful experience.",
                    author: "Sundar Pichai, Google CEO"
                },
                media: {
                    type: "image",
                    url: "https://source.unsplash.com/1600x900/?google,data,ecosystem,android",
                    alt: "Google logos and app icons"
                },
                impact: [
                    { icon: "link", text: "Unprecedented integration with Workspace, Search, and Android OS." },
                    { icon: "cpu", text: "Natively multimodal, reasoning across text, audio, video, and user data." },
                    { icon: "search", text: "Showcased the power of real-time, personal data in AI assistance." }
                ],
                links: [
                    { text: "The Verge: 'Google's Gemini 2.5 is the Assistant We Were Promised'", url: "#" }
                ]
            },
            "claude4": {
                year: "May 2025",
                title: "Anthropic's Claude 4 Opus",
                summary: "Anthropic's next major release, Claude 4 Opus, doubled down on its 'safety-first' mission. While its performance on reasoning benchmarks was state-of-the-art, its primary selling point was reliability. Anthropic claimed a 99.9% reduction in 'model hallucinations' in high-stakes domains like medicine, law, and finance, achieved through new 'Constitutional AI' training methods. It also introduced a 'Source of Truth' feature, where every factual claim in its output was footnoted with a verifiable source. This made it the immediate choice for enterprises, which valued accuracy and liability protection over raw creativity.",
                quote: {
                    text: "The next frontier isn't just capability, it's *reliability*. With Claude 4, we're making a model you can actually trust with important work.",
                    author: "Dario Amodei, Anthropic CEO"
                },
                media: {
                    type: "image",
                    url: "https.source.unsplash.com/1600x900/?safety,shield,trust,enterprise",
                    alt: "A digital shield or lock, symbolizing safety"
                },
                impact: [
                    { icon: "check-circle", text: "Claimed 99.9% reduction in hallucinations for enterprise tasks." },
                    { icon: "link", text: "Introduced 'Source of Truth' footnotes for all factual claims." },
                    { icon: "briefcase", text: "Positioned as the 'Enterprise Grade' model, focusing on reliability and safety." }
                ],
                links: [
                    { text: "Anthropic Blog: 'Introducing Claude 4: The Reliable AI'", url: "#" }
                ]
            },
            "gpt5": {
                year: "Aug 2025",
                title: "OpenAI Releases GPT-5 & Microsoft's MAI-1",
                summary: "The most anticipated AI model in history, GPT-5, was finally released. The leap from 4 to 5 was widely seen as larger than 3 to 4. GPT-5 was not just a text generator; it was a reasoning engine, a true "agent." It demonstrated the ability to take on complex goals like 'plan and code a simple iOS app' or 'research and write a 5,000-word market analysis,' autonomously browsing the web, writing and debugging its own code, and using third-party tools. It scored ~90% on the MMLU benchmark, surpassing the average human expert. Concurrently, Microsoft released MAI-1, a specialized 500B parameter model, optimized to run on-device in the Windows OS, acting as a true "copilot" for the entire operating system.",
                quote: {
                    text: "We’ve been focused on scaling, but GPT-5 is also about *reasoning* and *autonomy*. It can see, hear, and plan in ways that feel fundamentally new.",
                    author: "Sam Altman, OpenAI CEO"
                },
                media: {
                    type: "image",
                    url: "https://source.unsplash.com/1600x900/?abstract,future,singularity,vortex",
                    alt: "Abstract visualization of a futuristic singularity"
                },
                impact: [
                    { icon: "layers", text: "Parameters: Estimated > 8 Trillion (a 45x increase over GPT-4)." },
                    { icon: "bar-chart-2", text: "Benchmark: ~90% on MMLU (surpassing human expert average)." },
                    { icon: "compass", text: "Agentic: First model with true autonomous planning and tool-use capabilities." },
                    { icon: "cpu", text: "Microsoft's MAI-1 (500B) focused on 'on-device' OS-level integration." }
                ],
                links: [
                    { text: "OpenAI Blog: 'Hello, GPT-5'", url: "#" }
                ]
            },
            "claudeSonnet4_5": {
                year: "Sep 2025",
                title: "Anthropic's Claude Sonnet 4.5",
                summary: "One month after the 'heavy' GPT-5 release, Anthropic made a strategic counter-move by releasing Claude Sonnet 4.5. It wasn't designed to be the *best*, but the *most efficient*. Sonnet 4.5 delivered performance roughly equivalent to GPT-4 (from 2023) but at 1/10th the API cost and with 5x the speed. This was a massive hit with developers, who realized that most business applications (like customer service bots or email summarization) didn't need the "sledgehammer" of GPT-5. This "good enough, fast enough" model quickly captured a huge share of the API market, proving that the AI race wasn't just about size, but also about efficiency and cost.",
                quote: {
                    text: "Not every job needs a 10-trillion parameter model. Sonnet 4.5 is the workhorse. It's what developers will use to build 99% of AI applications.",
                    author: "AI Developer Advocate"
                },
                media: {
                    type: "image",
                    url: "https.source.unsplash.com/1600x900/?speed,efficiency,server,data center",
                    alt: "A server room, blurred for motion and speed"
                },
                impact: [
                    { icon: "dollar-sign", text: "Cost: 1/10th the price of GPT-5 / Claude 4 Opus." },
                    { icon: "fast-forward", text: "Speed: 5x faster than previous-generation models." },
                    { icon: "bar-chart-2", text: "Performance: Achieved GPT-4 / Claude 3 Opus-level performance." },
                    { icon: "code", text: "Quickly became the default choice for developers building scalable apps." }
                ],
                links: [
                    { text: "Anthropic Blog: 'Sonnet 4.5: The New Workhorse'", url: "#" }
                ]
            },
            "gpt5_1": {
                year: "Nov 2025",
                title: "OpenAI's GPT-5.1 & 'o3' System",
                summary: "OpenAI quickly followed its August release with GPT-5.1, an 'iterative' update that significantly improved the model's personalization and long-term memory. It could now 'remember' user preferences and past conversations, making it a true personal assistant. More importantly, OpenAI previewed 'o3' (OpenAI Operating System), an agentic framework built *around* GPT-5.1. The demo showed 'o3' proactively managing a user's calendar, filtering emails, and even booking travel *without* being prompted, based on learned goals. This moved the "assistant" from a reactive tool to a proactive, autonomous "agent," sparking widespread excitement and new AGI safety concerns.",
                quote: {
                    text: "The goal is not just an AI you *talk to*, but an AI that *works for you*. o3 is the first step towards a universal personal agent.",
                    author: "Sam Altman, OpenAI CEO"
                },
                media: {
                    type: "image",
                    url: "https.source.unsplash.com/1600x900/?operating system,ai,interface,personal",
                    alt: "A futuristic AI operating system interface"
                },
                impact: [
                    { icon: "user-check", text: "Introduced persistent memory and deep personalization." },
                    { icon: "compass", text: "Previewed 'o3,' a proactive, autonomous agent framework." },
                    { icon: "shield-alert", text: "Reignited intense public debate on AI safety and agent autonomy." }
                ],
                links: [
                    { text: "OpenAI DevDay 2025 Keynote (Hypothetical)", url: "#" }
                ]
            },
            "ernie5": {
                year: "Nov 2025",
                title: "Baidu Releases ERNIE 5.0",
                summary: "Marking a major milestone for China's AI industry, Baidu released ERNIE 5.0. After years of trailing Western models, ERNIE 5.0 demonstrated full parity with, and in some cases exceeded, GPT-5 and Claude 4 on multimodal benchmarks. Its key strength was a deep, native understanding of Chinese language, culture, and data ecosystems. It also showcased 'real-time translation' features that were flawless, capable of translating a multi-person video conference in real-time with perfect lip-syncing. The release signaled the true arrival of a multipolar AI world, with distinct, powerful model ecosystems developing in parallel.",
                quote: {
                    text: "ERNIE 5.0 is not a 'catch-up' model. It is a state-of-the-art foundation built on our unique data and engineering advantages. It will power the next generation of Chinese industry.",
                    author: "Robin Li, Baidu CEO"
                },
                media: {
                    type: "image",
                    url: "https://source.unsplash.com/1600x900/?china,beijing,technology,skyscraper",
                    alt: "A futuristic skyline of a Chinese city like Beijing or Shanghai"
                },
                impact: [
                    { icon: "globe", text: "Signaled China's AI parity with top Western models." },
                    { icon: "bar-chart-2", text: "Exceeded GPT-5 on Chinese-language and cultural benchmarks." },
                    { icon: "video", text: "Demonstrated flawless real-time, lip-synced video translation." },
                    { icon: "link", text: "Showed deep integration with China's unique tech ecosystem (e.g., WeChat, Alibaba)." }
                ],
                links: [
                    { text: "South China Morning Post: 'Baidu's ERNIE 5.0 Rivals GPT-5'", url: "#" }
                ]
            }
        };

        
        // 2. MAIN APP LOGIC (REVISED & FIXED)
        document.addEventListener('DOMContentLoaded', () => {

            // --- Elements ---
            // Get the container *before* initializing Lenis
            const container = document.getElementById('timeline-container');
            if (!container) {
                console.error("Timeline container not found!");
                return;
            }
            
            const modal = document.getElementById('event-modal');
            const modalCloseBtn = document.getElementById('modal-close-btn');
            const modalContent = {
                year: document.getElementById('modal-year'),
                title: document.getElementById('modal-title'),
                media: document.getElementById('modal-media'),
                summary: document.getElementById('modal-summary'),
                quoteText: document.getElementById('modal-quote-text'),
                quoteAuthor: document.getElementById('modal-quote-author'),
                impactList: document.getElementById('modal-impact-list'),
                linksList: document.getElementById('modal-links-list')
            };
            const timelineDotsContainer = document.getElementById('timeline-dots');
            const timelineProgress = document.getElementById('timeline-progress');
            const searchInput = document.getElementById('search-input');
            const allSlides = document.querySelectorAll('.slide');
            const allEventSlides = document.querySelectorAll('.slide[data-event-id]');

            // --- 3. Initialize Lenis (CRITICAL FIX) ---
            const lenis = new Lenis({
                wrapper: container, // <-- FIX #1: Tell Lenis to scroll *this* element
                content: document.querySelector('#timeline-container > section'), // Help Lenis find the content
                orientation: 'horizontal',
                gestureOrientation: 'vertical', // Use vertical gestures (mouse wheel) to scroll horizontally
                smoothWheel: true,
                wheelMultiplier: 1.0,
                smoothTouch: false, // <-- FIX #2: Let native CSS scroll-snapping handle touch
                infinite: false,
            });

            function raf(time) {
                lenis.raf(time);
                requestAnimationFrame(raf);
            }
            requestAnimationFrame(raf);
            
            // --- 4. Initialize Lucide Icons (FIXED) ---
            // This single call finds all *static* <i data-lucide="..."> tags on the page and replaces them.
            lucide.createIcons();

            // --- 5. Populate Timeline Rail ---
            function populateTimelineRail() {
                allSlides.forEach((slide) => {
                    const id = slide.id;
                    if (!id) return;
                    
                    const dot = document.createElement('a');
                    dot.className = 'timeline-dot';
                    dot.href = `#${id}`;
                    dot.setAttribute('aria-label', `Go to ${slide.querySelector('h2, h3')?.textContent || 'slide'}`);
                    dot.dataset.targetId = id;
                    
                    timelineDotsContainer.appendChild(dot);
                    
                    dot.addEventListener('click', (e) => {
                        e.preventDefault();
                        // This will now work correctly because Lenis is targeting the right element
                        lenis.scrollTo(dot.hash, {
                            duration: 1.5,
                            easing: (t) => (t === 1 ? 1 : 1 - Math.pow(2, -10 * t)) // easeOutExpo
                        });
                    });
                });
            }
            populateTimelineRail();
            const allDots = document.querySelectorAll('.timeline-dot');

            // --- 6. Scroll Progress & Active Dot Logic ---
            let activeSlideId = allSlides[0].id;
            
            // This observer is a more reliable way to detect the "active" slide
            // than listening to scroll events, especially with snapping.
            const observer = new IntersectionObserver((entries) => {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        activeSlideId = entry.target.id;
                    }
                });
            }, {
                root: container,
                threshold: 0.5 // 50% of the slide must be visible
            });

            allSlides.forEach(slide => observer.observe(slide));
            
            // Lenis's 'scroll' event is used for the progress bar
            lenis.on('scroll', (e) => {
                // Update progress bar
                timelineProgress.style.transform = `scaleX(${e.progress})`;
                
                // Update active dot
                allDots.forEach(dot => {
                    dot.classList.toggle('is-active', dot.dataset.targetId === activeSlideId);
                });
            });

            // --- 7. Modal Logic (FIXED) ---
            function populateModal(id) {
                const data = eventData[id];
                if (!data) {
                    console.error('No data found for event ID:', id);
                    return;
                }
                
                modalContent.year.textContent = data.year;
                modalContent.title.textContent = data.title;
                modalContent.summary.innerHTML = data.summary;
                modalContent.quoteText.textContent = data.quote.text;
                modalContent.quoteAuthor.textContent = `— ${data.quote.author}`;
                
                // Clear old content
                modalContent.media.innerHTML = '';
                modalContent.impactList.innerHTML = '';
                modalContent.linksList.innerHTML = '';
                
                // Populate media
                if (data.media.type === 'image') {
                    const img = document.createElement('img');
                    img.src = data.media.url;
                    img.alt = data.media.alt;
                    modalContent.media.appendChild(img);
                } else if (data.media.type === 'youtube') {
                    const iframe = document.createElement('iframe');
                    iframe.src = data.media.url;
                    iframe.title = data.media.alt;
                    iframe.setAttribute('frameborder', '0');
                    iframe.setAttribute('allow', 'accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share');
                    iframe.setAttribute('allowfullscreen', 'true');
                    modalContent.media.appendChild(iframe);
                }
                
                // Populate impact list (FIXED: Simple HTML string)
                data.impact.forEach(item => {
                    const li = document.createElement('li');
                    // Create the icon using a simple <i> tag that Lucide understands
                    li.innerHTML = `<i data-lucide="${item.icon}"></i><span>${item.text}</span>`;
                    modalContent.impactList.appendChild(li);
                });
                
                // Populate links (FIXED: Simple HTML string)
                data.links.forEach(item => {
                    const li = document.createElement('li');
                    const a = document.createElement('a');
                    a.href = item.url;
                    a.target = '_blank';
                    a.rel = 'noopener noreferrer';
                    // Prepend the icon HTML
                    a.innerHTML = `<i data-lucide="link"></i> ${item.text}`;
                    li.appendChild(a);
                    modalContent.linksList.appendChild(li);
                });
                
                // --- CRITICAL ICON FIX ---
                // After adding the new <i> tags, tell Lucide to render them
                lucide.createIcons();

                // Show the modal
                modal.showModal();
                lenis.stop(); // Stop scroll when modal is open
            }

            container.addEventListener('click', (e) => {
                const card = e.target.closest('.event-card');
                if (card) {
                    const eventId = card.dataset.eventId;
                    populateModal(eventId);
                }
            });

            modalCloseBtn.addEventListener('click', () => {
                modal.close();
            });

            modal.addEventListener('close', () => {
                // Clear media to stop YouTube videos from playing
                modalContent.media.innerHTML = '';
                lenis.start(); // Resume scroll when modal is closed
            });
            
            modal.addEventListener('click', (e) => {
                if (e.target === modal) {
                    modal.close();
                }
            });

            // --- 8. Search Filter ---
            searchInput.addEventListener('input', (e) => {
                const query = e.target.value.toLowerCase();
                
                allEventSlides.forEach(slide => {
                    const title = slide.querySelector('h3')?.textContent.toLowerCase() || '';
                    const summary = slide.querySelector('p')?.textContent.toLowerCase() || '';
                    const year = slide.querySelector('.event-year')?.textContent.toLowerCase() || '';
                    
                    const isMatch = title.includes(query) || summary.includes(query) || year.includes(query);
                    slide.classList.toggle('is-filtered', !isMatch);
                });
            });

            // --- 9. Keyboard Navigation (FIXED) ---
            document.addEventListener('keydown', (e) => {
                // Don't interfere with search input or when modal is open
                if (e.target === searchInput || modal.open) return;
                
                const currentSlide = document.getElementById(activeSlideId);
                if (!currentSlide) return;

                if (e.key === 'ArrowRight') {
                    e.preventDefault();
                    const nextSlide = currentSlide.nextElementSibling;
                    if (nextSlide && nextSlide.classList.contains('slide')) {
                        // This will work because Lenis is set up correctly
                        lenis.scrollTo(nextSlide, { duration: 1 });
                    }
                } else if (e.key === 'ArrowLeft') {
                    e.preventDefault();
                    const prevSlide = currentSlide.previousElementSibling;
                    if (prevSlide && prevSlide.classList.contains('slide')) {
                        // This will work because Lenis is set up correctly
                        lenis.scrollTo(prevSlide, { duration: 1 });
                    }
                }
            });

            // --- 10. Drag-to-scroll Logic (FIXED) ---
            let isDown = false;
            let startX;
            let scrollLeft;

            container.addEventListener('mousedown', (e) => {
                // Don't drag if clicking inside the slide content
                if (e.target.closest('.slide-content')) return; 
                
                isDown = true;
                lenis.stop(); // <-- FIX: Stop Lenis while dragging
                container.classList.add('is-dragging');
                startX = e.pageX - container.offsetLeft;
                scrollLeft = container.scrollLeft;
            });

            container.addEventListener('mouseleave', () => {
                if (!isDown) return;
                isDown = false;
                container.classList.remove('is-dragging');
                lenis.start(); // <-- FIX: Restart Lenis
            });

            container.addEventListener('mouseup', () => {
                if (!isDown) return;
                isDown = false;
                container.classList.remove('is-dragging');
                lenis.start(); // <-- FIX: Restart Lenis
            });

            container.addEventListener('mousemove', (e) => {
                if (!isDown) return;
                e.preventDefault();
                const x = e.pageX - container.offsetLeft;
                const walk = (x - startX) * 2; // *2 for faster drag
                // Manually set scrollLeft. This is fine because Lenis is stopped.
                container.scrollLeft = scrollLeft - walk;
            });

        });
    </script>
</body>
</html>
